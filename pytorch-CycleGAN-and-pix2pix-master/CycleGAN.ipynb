{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CycleGAN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/bkkaggle/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"5VIGyIus8Vr7","colab_type":"text"},"source":["Take a look at the [repository](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) for more information"]},{"cell_type":"markdown","metadata":{"id":"7wNjDKdQy35h","colab_type":"text"},"source":["# Install"]},{"cell_type":"code","metadata":{"id":"TRm-USlsHgEV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"3036bea0-1747-4604-cbe3-c648b9a9989d","executionInfo":{"status":"ok","timestamp":1577710292301,"user_tz":-480,"elapsed":7463,"user":{"displayName":"平星磊","photoUrl":"","userId":"16521551903901378041"}}},"source":["!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'pytorch-CycleGAN-and-pix2pix'...\n","remote: Enumerating objects: 2194, done.\u001b[K\n","remote: Total 2194 (delta 0), reused 0 (delta 0), pack-reused 2194\u001b[K\n","Receiving objects: 100% (2194/2194), 8.02 MiB | 13.23 MiB/s, done.\n","Resolving deltas: 100% (1413/1413), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pt3igws3eiVp","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('pytorch-CycleGAN-and-pix2pix/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z1EySlOXwwoa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":663},"outputId":"0834038d-3fcd-428b-ba9b-d2d4ae531e1e","executionInfo":{"status":"ok","timestamp":1577710314819,"user_tz":-480,"elapsed":13847,"user":{"displayName":"平星磊","photoUrl":"","userId":"16521551903901378041"}}},"source":["!pip install -r requirements.txt"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.3.1)\n","Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.4.2)\n","Collecting dominate>=2.3.1\n","  Downloading https://files.pythonhosted.org/packages/1d/64/593e829416c951eb35c2246430d59b86f640087e29e71f32632bcde5d0f7/dominate-2.4.0-py2.py3-none-any.whl\n","Collecting visdom>=0.1.8.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/75/e078f5a2e1df7e0d3044749089fc2823e62d029cc027ed8ae5d71fafcbdc/visdom-0.1.8.9.tar.gz (676kB)\n","\r\u001b[K     |▌                               | 10kB 27.2MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 17.6MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 23.1MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 6.3MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51kB 7.4MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 8.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81kB 8.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92kB 9.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 112kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 143kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 174kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 194kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 204kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 225kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 235kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 256kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 266kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 286kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 307kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 327kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 348kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 358kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 368kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 378kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 389kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 399kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 409kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 419kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 430kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 440kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 450kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 460kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 471kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 481kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 491kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 501kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 512kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 522kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 532kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 542kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 552kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 563kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 573kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 583kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 593kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 604kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 614kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 624kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 634kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 645kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 655kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 665kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 675kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 686kB 10.0MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->-r requirements.txt (line 1)) (1.17.4)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.2.1->-r requirements.txt (line 2)) (4.3.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.2.1->-r requirements.txt (line 2)) (1.12.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from visdom>=0.1.8.3->-r requirements.txt (line 4)) (1.3.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from visdom>=0.1.8.3->-r requirements.txt (line 4)) (2.21.0)\n","Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom>=0.1.8.3->-r requirements.txt (line 4)) (4.5.3)\n","Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom>=0.1.8.3->-r requirements.txt (line 4)) (17.0.0)\n","Collecting jsonpatch\n","  Downloading https://files.pythonhosted.org/packages/86/7e/035d19a73306278673039f0805b863be8798057cc1b4008b9c8c7d1d32a3/jsonpatch-1.24-py2.py3-none-any.whl\n","Collecting torchfile\n","  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\n","Collecting websocket-client\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n","\u001b[K     |████████████████████████████████| 204kB 23.3MB/s \n","\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision>=0.2.1->-r requirements.txt (line 2)) (0.46)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->visdom>=0.1.8.3->-r requirements.txt (line 4)) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->visdom>=0.1.8.3->-r requirements.txt (line 4)) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->visdom>=0.1.8.3->-r requirements.txt (line 4)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->visdom>=0.1.8.3->-r requirements.txt (line 4)) (2019.11.28)\n","Collecting jsonpointer>=1.9\n","  Downloading https://files.pythonhosted.org/packages/18/b0/a80d29577c08eea401659254dfaed87f1af45272899e1812d7e01b679bc5/jsonpointer-2.0-py2.py3-none-any.whl\n","Building wheels for collected packages: visdom, torchfile\n","  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for visdom: filename=visdom-0.1.8.9-cp36-none-any.whl size=655252 sha256=26aa295ae64656cb859cc10ff2da8661e8c6ffc61d3f3417ad690d1328bccc7f\n","  Stored in directory: /root/.cache/pip/wheels/70/19/a7/6d589ed967f4dfefd33bc166d081257bd4ed0cb618dccfd62a\n","  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchfile: filename=torchfile-0.1.0-cp36-none-any.whl size=5711 sha256=d03400be2071896cc4c005412325d11eb091300834ccdfe410c3b8160c460507\n","  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\n","Successfully built visdom torchfile\n","Installing collected packages: dominate, jsonpointer, jsonpatch, torchfile, websocket-client, visdom\n","Successfully installed dominate-2.4.0 jsonpatch-1.24 jsonpointer-2.0 torchfile-0.1.0 visdom-0.1.8.9 websocket-client-0.57.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8daqlgVhw29P","colab_type":"text"},"source":["# Datasets\n","\n","Download one of the official datasets with:\n","\n","-   `bash ./datasets/download_cyclegan_dataset.sh [apple2orange, orange2apple, summer2winter_yosemite, winter2summer_yosemite, horse2zebra, zebra2horse, monet2photo, style_monet, style_cezanne, style_ukiyoe, style_vangogh, sat2map, map2sat, cityscapes_photo2label, cityscapes_label2photo, facades_photo2label, facades_label2photo, iphone2dslr_flower]`\n","\n","Or use your own dataset by creating the appropriate folders and adding in the images.\n","\n","-   Create a dataset folder under `/dataset` for your dataset.\n","-   Create subfolders `testA`, `testB`, `trainA`, and `trainB` under your dataset's folder. Place any images you want to transform from a to b (cat2dog) in the `testA` folder, images you want to transform from b to a (dog2cat) in the `testB` folder, and do the same for the `trainA` and `trainB` folders."]},{"cell_type":"code","metadata":{"id":"vrdOettJxaCc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3f427142-33a6-4be4-9d1d-6516f5056c74","executionInfo":{"status":"ok","timestamp":1577710465781,"user_tz":-480,"elapsed":2201,"user":{"displayName":"平星磊","photoUrl":"","userId":"16521551903901378041"}}},"source":["!bash ./datasets/download_cyclegan_dataset.sh style_vangogh"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Available datasets are: apple2orange, summer2winter_yosemite, horse2zebra, monet2photo, cezanne2photo, ukiyoe2photo, vangogh2photo, maps, cityscapes, facades, iphone2dslr_flower, ae_photos\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gdUz4116xhpm","colab_type":"text"},"source":["# Pretrained models\n","\n","Download one of the official pretrained models with:\n","\n","-   `bash ./scripts/download_cyclegan_model.sh [apple2orange, orange2apple, summer2winter_yosemite, winter2summer_yosemite, horse2zebra, zebra2horse, monet2photo, style_monet, style_cezanne, style_ukiyoe, style_vangogh, sat2map, map2sat, cityscapes_photo2label, cityscapes_label2photo, facades_photo2label, facades_label2photo, iphone2dslr_flower]`\n","\n","Or add your own pretrained model to `./checkpoints/{NAME}_pretrained/latest_net_G.pt`"]},{"cell_type":"code","metadata":{"id":"B75UqtKhxznS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":309},"outputId":"235fdf56-0fc6-49c7-fd61-6e7fa8a7d960","executionInfo":{"status":"ok","timestamp":1577710474527,"user_tz":-480,"elapsed":2661,"user":{"displayName":"平星磊","photoUrl":"","userId":"16521551903901378041"}}},"source":["!bash ./scripts/download_cyclegan_model.sh style_vangogh"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Note: available models are apple2orange, orange2apple, summer2winter_yosemite, winter2summer_yosemite, horse2zebra, zebra2horse, monet2photo, style_monet, style_cezanne, style_ukiyoe, style_vangogh, sat2map, map2sat, cityscapes_photo2label, cityscapes_label2photo, facades_photo2label, facades_label2photo, iphone2dslr_flower\n","Specified [style_vangogh]\n","WARNING: timestamping does nothing in combination with -O. See the manual\n","for details.\n","\n","--2019-12-30 12:54:33--  http://efrosgans.eecs.berkeley.edu/cyclegan/pretrained_models/style_vangogh.pth\n","Resolving efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)... 128.32.189.73\n","Connecting to efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)|128.32.189.73|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 45575747 (43M)\n","Saving to: ‘./checkpoints/style_vangogh_pretrained/latest_net_G.pth’\n","\n","./checkpoints/style 100%[===================>]  43.46M  67.6MB/s    in 0.6s    \n","\n","2019-12-30 12:54:33 (67.6 MB/s) - ‘./checkpoints/style_vangogh_pretrained/latest_net_G.pth’ saved [45575747/45575747]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yFw1kDQBx3LN","colab_type":"text"},"source":["# Training\n","\n","-   `python train.py --dataroot ./datasets/horse2zebra --name horse2zebra --model cycle_gan`\n","\n","Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. I've found that a batch size of 16 fits onto 4 V100s and can finish training an epoch in ~90s.\n","\n","Once your model has trained, copy over the last checkpoint to a format that the testing model can automatically detect:\n","\n","Use `cp ./checkpoints/horse2zebra/latest_net_G_A.pth ./checkpoints/horse2zebra/latest_net_G.pth` if you want to transform images from class A to class B and `cp ./checkpoints/horse2zebra/latest_net_G_B.pth ./checkpoints/horse2zebra/latest_net_G.pth` if you want to transform images from class B to class A.\n"]},{"cell_type":"code","metadata":{"id":"0sp7TCT2x9dB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"3ca3e97a-9855-4c75-f74a-392ac2bf6bb5","executionInfo":{"status":"ok","timestamp":1577711318014,"user_tz":-480,"elapsed":841093,"user":{"displayName":"平星磊","photoUrl":"","userId":"16521551903901378041"}}},"source":["!python train.py --dataroot ./datasets/style_vangogh --name style_vangogh --model cycle_gan"],"execution_count":11,"outputs":[{"output_type":"stream","text":["----------------- Options ---------------\n","               batch_size: 1                             \n","                    beta1: 0.5                           \n","          checkpoints_dir: ./checkpoints                 \n","           continue_train: False                         \n","                crop_size: 256                           \n","                 dataroot: ./datasets/horse2zebra        \t[default: None]\n","             dataset_mode: unaligned                     \n","                direction: AtoB                          \n","              display_env: main                          \n","             display_freq: 400                           \n","               display_id: 1                             \n","            display_ncols: 4                             \n","             display_port: 8097                          \n","           display_server: http://localhost              \n","          display_winsize: 256                           \n","                    epoch: latest                        \n","              epoch_count: 1                             \n","                 gan_mode: lsgan                         \n","                  gpu_ids: 0                             \n","                init_gain: 0.02                          \n","                init_type: normal                        \n","                 input_nc: 3                             \n","                  isTrain: True                          \t[default: None]\n","                 lambda_A: 10.0                          \n","                 lambda_B: 10.0                          \n","          lambda_identity: 0.5                           \n","                load_iter: 0                             \t[default: 0]\n","                load_size: 286                           \n","                       lr: 0.0002                        \n","           lr_decay_iters: 50                            \n","                lr_policy: linear                        \n","         max_dataset_size: inf                           \n","                    model: cycle_gan                     \n","                 n_epochs: 100                           \n","           n_epochs_decay: 100                           \n","               n_layers_D: 3                             \n","                     name: style_vangogh                 \t[default: experiment_name]\n","                      ndf: 64                            \n","                     netD: basic                         \n","                     netG: resnet_9blocks                \n","                      ngf: 64                            \n","               no_dropout: True                          \n","                  no_flip: False                         \n","                  no_html: False                         \n","                     norm: instance                      \n","              num_threads: 4                             \n","                output_nc: 3                             \n","                    phase: train                         \n","                pool_size: 50                            \n","               preprocess: resize_and_crop               \n","               print_freq: 100                           \n","             save_by_iter: False                         \n","          save_epoch_freq: 5                             \n","         save_latest_freq: 5000                          \n","           serial_batches: False                         \n","                   suffix:                               \n","         update_html_freq: 1000                          \n","                  verbose: False                         \n","----------------- End -------------------\n","dataset [UnalignedDataset] was created\n","The number of training images = 1334\n","initialize network with normal\n","initialize network with normal\n","initialize network with normal\n","initialize network with normal\n","model [CycleGANModel] was created\n","---------- Networks initialized -------------\n","[Network G_A] Total number of parameters : 11.378 M\n","[Network G_B] Total number of parameters : 11.378 M\n","[Network D_A] Total number of parameters : 2.765 M\n","[Network D_B] Total number of parameters : 2.765 M\n","-----------------------------------------------\n","Setting up a new session...\n","create web directory ./checkpoints/style_vangogh/web...\n","(epoch: 1, iters: 100, time: 0.316, data: 0.169) D_A: 0.423 G_A: 0.271 cycle_A: 3.708 idt_A: 2.034 D_B: 0.367 G_B: 0.305 cycle_B: 4.265 idt_B: 1.891 \n","(epoch: 1, iters: 200, time: 0.318, data: 0.001) D_A: 0.516 G_A: 0.349 cycle_A: 2.197 idt_A: 1.045 D_B: 0.327 G_B: 0.329 cycle_B: 2.274 idt_B: 1.027 \n","(epoch: 1, iters: 300, time: 0.317, data: 0.001) D_A: 0.236 G_A: 0.392 cycle_A: 2.558 idt_A: 1.188 D_B: 0.277 G_B: 0.380 cycle_B: 2.502 idt_B: 1.079 \n","(epoch: 1, iters: 400, time: 0.647, data: 0.001) D_A: 0.419 G_A: 0.273 cycle_A: 2.168 idt_A: 0.938 D_B: 0.264 G_B: 0.216 cycle_B: 2.017 idt_B: 0.973 \n","(epoch: 1, iters: 500, time: 0.316, data: 0.001) D_A: 0.282 G_A: 0.361 cycle_A: 4.096 idt_A: 1.347 D_B: 0.317 G_B: 0.567 cycle_B: 3.020 idt_B: 2.063 \n","(epoch: 1, iters: 600, time: 0.316, data: 0.001) D_A: 0.223 G_A: 0.351 cycle_A: 1.095 idt_A: 1.993 D_B: 0.253 G_B: 0.471 cycle_B: 4.288 idt_B: 0.625 \n","(epoch: 1, iters: 700, time: 0.316, data: 0.001) D_A: 0.397 G_A: 0.275 cycle_A: 1.667 idt_A: 1.878 D_B: 0.177 G_B: 0.399 cycle_B: 4.886 idt_B: 0.712 \n","(epoch: 1, iters: 800, time: 0.493, data: 0.001) D_A: 0.189 G_A: 0.289 cycle_A: 1.850 idt_A: 0.785 D_B: 0.353 G_B: 0.696 cycle_B: 1.704 idt_B: 0.746 \n","(epoch: 1, iters: 900, time: 0.316, data: 0.002) D_A: 0.209 G_A: 0.204 cycle_A: 1.849 idt_A: 0.650 D_B: 0.204 G_B: 0.372 cycle_B: 1.671 idt_B: 0.873 \n","(epoch: 1, iters: 1000, time: 0.317, data: 0.002) D_A: 0.310 G_A: 0.149 cycle_A: 1.776 idt_A: 1.266 D_B: 0.236 G_B: 0.193 cycle_B: 2.616 idt_B: 0.772 \n","(epoch: 1, iters: 1100, time: 0.318, data: 0.001) D_A: 0.297 G_A: 0.170 cycle_A: 1.979 idt_A: 0.958 D_B: 0.377 G_B: 0.386 cycle_B: 2.251 idt_B: 0.742 \n","(epoch: 1, iters: 1200, time: 0.496, data: 0.001) D_A: 0.247 G_A: 0.459 cycle_A: 2.031 idt_A: 0.891 D_B: 0.123 G_B: 0.390 cycle_B: 2.064 idt_B: 1.046 \n","(epoch: 1, iters: 1300, time: 0.318, data: 0.001) D_A: 0.184 G_A: 0.352 cycle_A: 1.833 idt_A: 1.246 D_B: 0.236 G_B: 0.362 cycle_B: 2.361 idt_B: 0.756 \n","End of epoch 1 / 200 \t Time Taken: 425 sec\n","learning rate = 0.0002000\n","(epoch: 2, iters: 66, time: 0.316, data: 0.002) D_A: 0.300 G_A: 0.472 cycle_A: 1.580 idt_A: 0.867 D_B: 0.402 G_B: 0.343 cycle_B: 1.684 idt_B: 0.918 \n","(epoch: 2, iters: 166, time: 0.317, data: 0.001) D_A: 0.219 G_A: 0.563 cycle_A: 1.614 idt_A: 0.949 D_B: 0.230 G_B: 0.447 cycle_B: 2.193 idt_B: 0.735 \n","(epoch: 2, iters: 266, time: 0.704, data: 0.002) D_A: 0.231 G_A: 0.749 cycle_A: 1.674 idt_A: 2.120 D_B: 0.149 G_B: 0.300 cycle_B: 4.329 idt_B: 0.734 \n","(epoch: 2, iters: 366, time: 0.316, data: 0.001) D_A: 0.195 G_A: 0.415 cycle_A: 1.521 idt_A: 1.022 D_B: 0.294 G_B: 0.273 cycle_B: 1.922 idt_B: 0.544 \n","(epoch: 2, iters: 466, time: 0.316, data: 0.001) D_A: 0.291 G_A: 0.210 cycle_A: 1.473 idt_A: 1.206 D_B: 0.269 G_B: 0.320 cycle_B: 2.454 idt_B: 0.711 \n","(epoch: 2, iters: 566, time: 0.318, data: 0.001) D_A: 0.322 G_A: 0.119 cycle_A: 1.593 idt_A: 1.034 D_B: 0.329 G_B: 0.110 cycle_B: 2.059 idt_B: 0.774 \n","(epoch: 2, iters: 666, time: 0.646, data: 0.001) D_A: 0.205 G_A: 0.349 cycle_A: 2.596 idt_A: 1.081 D_B: 0.184 G_B: 0.237 cycle_B: 2.556 idt_B: 1.195 \n","(epoch: 2, iters: 766, time: 0.317, data: 0.001) D_A: 0.198 G_A: 0.503 cycle_A: 3.535 idt_A: 0.683 D_B: 0.322 G_B: 0.491 cycle_B: 1.615 idt_B: 1.661 \n","(epoch: 2, iters: 866, time: 0.317, data: 0.001) D_A: 0.154 G_A: 0.215 cycle_A: 2.030 idt_A: 1.262 D_B: 0.308 G_B: 0.695 cycle_B: 3.582 idt_B: 0.805 \n","(epoch: 2, iters: 966, time: 0.317, data: 0.001) D_A: 0.255 G_A: 0.440 cycle_A: 3.243 idt_A: 0.565 D_B: 0.217 G_B: 0.361 cycle_B: 1.151 idt_B: 1.275 \n","(epoch: 2, iters: 1066, time: 0.501, data: 0.001) D_A: 0.372 G_A: 0.174 cycle_A: 6.208 idt_A: 0.570 D_B: 0.226 G_B: 0.554 cycle_B: 1.438 idt_B: 2.748 \n","(epoch: 2, iters: 1166, time: 0.316, data: 0.001) D_A: 0.433 G_A: 0.676 cycle_A: 1.567 idt_A: 0.583 D_B: 0.087 G_B: 0.750 cycle_B: 1.335 idt_B: 0.802 \n","(epoch: 2, iters: 1266, time: 0.315, data: 0.001) D_A: 0.188 G_A: 0.359 cycle_A: 1.537 idt_A: 0.763 D_B: 0.497 G_B: 1.071 cycle_B: 1.552 idt_B: 0.567 \n","Traceback (most recent call last):\n","  File \"train.py\", line 52, in <module>\n","    model.optimize_parameters()   # calculate loss functions, get gradients, update network weights\n","  File \"/content/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py\", line 192, in optimize_parameters\n","    self.backward_D_A()      # calculate gradients for D_A\n","  File \"/content/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py\", line 144, in backward_D_A\n","    self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)\n","  File \"/content/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py\", line 132, in backward_D_basic\n","    loss_D_real = self.criterionGAN(pred_real, True)\n","  File \"/content/pytorch-CycleGAN-and-pix2pix/models/networks.py\", line 269, in __call__\n","    loss = self.loss(prediction, target_tensor)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 541, in __call__\n","    result = self.forward(*input, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\", line 431, in forward\n","    return F.mse_loss(input, target, reduction=self.reduction)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 2204, in mse_loss\n","    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9UkcaFZiyASl","colab_type":"text"},"source":["# Testing\n","\n","-   `python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout`\n","\n","Change the `--dataroot` and `--name` to be consistent with your trained model's configuration.\n","\n","> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\n","> The option --model test is used for generating results of CycleGAN only for one side. This option will automatically set --dataset_mode single, which only loads the images from one set. On the contrary, using --model cycle_gan requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at ./results/. Use --results_dir {directory_path_to_save_result} to specify the results directory.\n","\n","> For your own experiments, you might want to specify --netG, --norm, --no_dropout to match the generator architecture of the trained model."]},{"cell_type":"code","metadata":{"id":"uCsKkEq0yGh0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"a24e236f-9728-4553-f1ff-4827eafbaa41","executionInfo":{"status":"ok","timestamp":1577710459838,"user_tz":-480,"elapsed":59171,"user":{"displayName":"平星磊","photoUrl":"","userId":"16521551903901378041"}}},"source":["!python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout"],"execution_count":7,"outputs":[{"output_type":"stream","text":["----------------- Options ---------------\n","             aspect_ratio: 1.0                           \n","               batch_size: 1                             \n","          checkpoints_dir: ./checkpoints                 \n","                crop_size: 256                           \n","                 dataroot: datasets/horse2zebra/testA    \t[default: None]\n","             dataset_mode: single                        \n","                direction: AtoB                          \n","          display_winsize: 256                           \n","                    epoch: latest                        \n","                     eval: False                         \n","                  gpu_ids: 0                             \n","                init_gain: 0.02                          \n","                init_type: normal                        \n","                 input_nc: 3                             \n","                  isTrain: False                         \t[default: None]\n","                load_iter: 0                             \t[default: 0]\n","                load_size: 256                           \n","         max_dataset_size: inf                           \n","                    model: test                          \n","             model_suffix:                               \n","               n_layers_D: 3                             \n","                     name: horse2zebra_pretrained        \t[default: experiment_name]\n","                      ndf: 64                            \n","                     netD: basic                         \n","                     netG: resnet_9blocks                \n","                      ngf: 64                            \n","               no_dropout: True                          \t[default: False]\n","                  no_flip: False                         \n","                     norm: instance                      \n","                    ntest: inf                           \n","                 num_test: 50                            \n","              num_threads: 4                             \n","                output_nc: 3                             \n","                    phase: test                          \n","               preprocess: resize_and_crop               \n","              results_dir: ./results/                    \n","           serial_batches: False                         \n","                   suffix:                               \n","                  verbose: False                         \n","----------------- End -------------------\n","dataset [SingleDataset] was created\n","initialize network with normal\n","model [TestModel] was created\n","loading the model from ./checkpoints/horse2zebra_pretrained/latest_net_G.pth\n","---------- Networks initialized -------------\n","[Network G] Total number of parameters : 11.378 M\n","-----------------------------------------------\n","creating web directory ./results/horse2zebra_pretrained/test_latest\n","processing (0000)-th image... ['datasets/horse2zebra/testA/n02381460_1000.jpg']\n","processing (0005)-th image... ['datasets/horse2zebra/testA/n02381460_1110.jpg']\n","processing (0010)-th image... ['datasets/horse2zebra/testA/n02381460_1260.jpg']\n","processing (0015)-th image... ['datasets/horse2zebra/testA/n02381460_1420.jpg']\n","processing (0020)-th image... ['datasets/horse2zebra/testA/n02381460_1690.jpg']\n","processing (0025)-th image... ['datasets/horse2zebra/testA/n02381460_1830.jpg']\n","processing (0030)-th image... ['datasets/horse2zebra/testA/n02381460_2050.jpg']\n","processing (0035)-th image... ['datasets/horse2zebra/testA/n02381460_2460.jpg']\n","processing (0040)-th image... ['datasets/horse2zebra/testA/n02381460_2870.jpg']\n","processing (0045)-th image... ['datasets/horse2zebra/testA/n02381460_3040.jpg']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OzSKIPUByfiN","colab_type":"text"},"source":["# Visualize"]},{"cell_type":"code","metadata":{"id":"9Mgg8raPyizq","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","img = plt.imread('./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_fake.png')\n","plt.imshow(img)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0G3oVH9DyqLQ","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","img = plt.imread('./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_real.png')\n","plt.imshow(img)"],"execution_count":0,"outputs":[]}]}